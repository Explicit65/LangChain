{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with LangChain and Open AI\n",
    "\n",
    "Outline:\n",
    "- Setup LangChain, LangSmith and LangServe.\n",
    "- Use the basic and common component of LangChain, prompt template, models and output parser.\n",
    "- Build a simple application with LangChain.\n",
    "- Trace the application with LangSmith.\n",
    "- Serve the application with LangServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# For langchain to work, you need to have the following environment variables set:\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10c259cc0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10c25bd90>, root_client=<openai.OpenAI object at 0x103c6ed10>, root_async_client=<openai.AsyncOpenAI object at 0x10c259d20>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output parser\n",
    "result = llm.invoke(\"Who is the president of Nigeria?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='As of the latest available information, Bola Ahmed Tinubu is the President of Nigeria. He assumed office on May 29, 2023.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 14, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4691090a87', 'finish_reason': 'stop', 'logprobs': None} id='run-268e0d8a-b92f-4628-a030-4173eedb36de-0' usage_metadata={'input_tokens': 14, 'output_tokens': 30, 'total_tokens': 44, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = llm.invoke(\"What is mars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Mars is the fourth planet from the Sun in our solar system and is often referred to as the \"Red Planet\" due to its reddish appearance, which is caused by iron oxide, or rust, on its surface. It is a terrestrial planet with a thin atmosphere composed mostly of carbon dioxide, and it has surface features both similar to those found on Earth and the Moon, including valleys, deserts, and polar ice caps.\\n\\nMars has two small moons, Phobos and Deimos, which are thought to be captured asteroids. The planet has been a significant focus of space exploration, with numerous missions conducted by various space agencies to study its geology, climate, potential for past or present life, and its suitability for future human colonization. Notably, Mars hosts the tallest volcano and the deepest, longest canyon in the solar system, Olympus Mons and Valles Marineris, respectively.\\n\\nThe exploration of Mars is driven by its potential to have supported microbial life in the past, and it continues to be a major target for scientific research and exploration efforts.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 211, 'prompt_tokens': 10, 'total_tokens': 221, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_4691090a87', 'finish_reason': 'stop', 'logprobs': None}, id='run-9394e48e-7e62-4ae7-86d4-2c99085b9fb4-0', usage_metadata={'input_tokens': 10, 'output_tokens': 211, 'total_tokens': 221, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Act like an expert AI Engineer, Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt =  ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Act like an expert AI Engineer, Provide me answers based on the questions\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='As of my last training cutoff in October 2023, I don\\'t have any specific information about products or tools named \"Langsmith\" and \"Langserve\" within the realm of AI and technology. It\\'s possible that these could be newer tools developed after my last update, or they could be niche solutions not widely covered in the information I\\'ve been trained on. \\n\\nIf these terms refer to recent developments, I\\'d recommend checking the latest resources, such as product websites, recent AI conference papers, or industry news articles for the most up-to-date information. Alternatively, they could be hypothetical or conceptual terms that might not be officially recognized or documented widely. If you provide more context, I could potentially be more helpful.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 142, 'prompt_tokens': 34, 'total_tokens': 176, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None} id='run-2fe3c210-3a4f-45f9-a5fc-863909874ff7-0' usage_metadata={'input_tokens': 34, 'output_tokens': 142, 'total_tokens': 176, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "## Chain prompt with LLM\n",
    "chain = prompt|llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"Tell me about langsmith and langserve.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Generative Adversarial Networks (GANs) are a class of machine learning models introduced by Ian Goodfellow and his colleagues in 2014. They are primarily used for generating synthetic data that resemble real data distributions and are composed of two neural networks: the generator and the discriminator, which are trained simultaneously through an adversarial process.\\n\\n1. **Generator**: The generator's role is to produce data that mimic real data. It starts with random noise as input and generates data through a series of transformations. The objective of the generator is to improve over time such that the data it generates is indistinguishable from real data.\\n\\n2. **Discriminator**: The discriminator acts as a judge that evaluates the authenticity of the data. It receives input either from the real data distribution or from the generator and outputs a probability indicating how real or fake the input data is. The discriminator’s goal is to correctly distinguish between real data and generated data.\\n\\n3. **Adversarial Process**: During training, the generator tries to produce data that can fool the discriminator, while the discriminator simultaneously strives to increase its accuracy in distinguishing real from fake data. This creates a zero-sum game where improvements in one network lead to adjustments in the other.\\n\\n4. **Loss Functions**: The standard loss function for GANs involves a minimax objective where the discriminator is trained to maximize log probability of assigning the correct labels to real and fake data, while the generator is trained to minimize the log probability of the discriminator being correct. The loss functions usually are:\\n\\n   \\\\[ \\\\text{Discriminator Loss} = - \\\\log(D(x)) - \\\\log(1 - D(G(z))) \\\\]\\n   \\\\[ \\\\text{Generator Loss} = - \\\\log(D(G(z))) \\\\]\\n\\n   here, \\\\( x \\\\) is the real data, \\\\( z \\\\) is the noise input, \\\\( G(z) \\\\) is the generated data, and \\\\( D(\\\\cdot) \\\\) is the discriminator's prediction.\\n\\n5. **Applications**: GANs have been widely used in various applications, including image generation, style transfer, text-to-image synthesis, image inpainting, super-resolution, and even in areas like music and video generation.\\n\\n6. **Variations and Enhancements**: Numerous variations and enhancements have been developed since their inception, addressing challenges such as mode collapse, convergence difficulties, and instability during training. Some popular variants include Deep Convolutional GANs (DCGANs), Wasserstein GANs (WGANs), and Conditional GANs (cGANs).\\n\\nGANs are considered one of the most intriguing advancements in the field of deep learning, particularly due to their ability to create hyper-realistic synthetic data. However, they also present challenges in their training process, typically requiring careful tuning of hyperparameters and model architectures.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 572, 'prompt_tokens': 35, 'total_tokens': 607, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None} id='run-11da0a4c-d677-41b3-81d5-cbad6a17bf22-0' usage_metadata={'input_tokens': 35, 'output_tokens': 572, 'total_tokens': 607, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"Tell me about Generative Adversarial Network.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational Autoencoders (VAEs) are a type of generative model and an extension of traditional autoencoders used for unsupervised learning. Introduced by Kingma and Welling in 2013, VAEs are designed to learn a latent variable model for the data, where the latent variables follow a known distribution, typically a Gaussian distribution. They are particularly powerful for generating new data samples that resemble the original dataset.\n",
      "\n",
      "Here's a brief overview of key concepts related to VAEs:\n",
      "\n",
      "1. **Components**: Like a standard autoencoder, a VAE consists of two main parts: an encoder and a decoder.\n",
      "   - **Encoder**: Maps the input data to a latent space, representing it as a distribution. Instead of mapping inputs to a fixed vector, VAEs map inputs to a distribution, typically parameterized as a mean and a variance that define a Gaussian.\n",
      "   - **Decoder**: Reconstructs the input data from latent variables sampled from the distribution generated by the encoder.\n",
      "\n",
      "2. **Latent Space Representation**: Unlike traditional autoencoders, VAEs learn to express each input as a distribution. The encoder’s output comprises a mean vector and a variance vector, which are used to sample latent variables. This stochastic element allows for more robust generative capabilities.\n",
      "\n",
      "3. **Reparameterization Trick**: To backpropagate through the sampling process during training, the reparameterization trick is used. It involves expressing the sampled vector \\( z \\) as:\n",
      "   \\[\n",
      "   z = \\mu + \\sigma \\cdot \\epsilon\n",
      "   \\]\n",
      "   where \\( \\mu \\) and \\( \\sigma \\) are the mean and standard deviation from the encoder, and \\( \\epsilon \\) is a random variable from a standard normal distribution. This allows gradients to propagate through the network.\n",
      "\n",
      "4. **Loss Function**: The VAE loss function consists of two components:\n",
      "   - **Reconstruction Loss**: Measures how well the decoder can reconstruct the original input from the latent space, often using Mean Squared Error (MSE) or Binary Cross-Entropy (BCE).\n",
      "   - **KL-Divergence Loss**: A regularization term that ensures the latent space distributions approximate a standard normal distribution. It measures the divergence between the learned latent variable distribution and the prior (typically a standard normal distribution).\n",
      "\n",
      "5. **Applications**: VAEs are useful in a range of applications, including data generation, dimensionality reduction, feature extraction, and as a pre-training mechanism for other models.\n",
      "\n",
      "Overall, VAEs stand out by introducing a probabilistic approach to learning generative models, allowing for the creation of diverse and high-quality synthetic data while maintaining a continuous and interpolatable latent space.\n"
     ]
    }
   ],
   "source": [
    "# Print out just the sting answer only\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt|llm|output_parser\n",
    "\n",
    "response2 = chain.invoke({\"input\": \"Tell me about Variation auto encodersk.\"})\n",
    "print(response2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
